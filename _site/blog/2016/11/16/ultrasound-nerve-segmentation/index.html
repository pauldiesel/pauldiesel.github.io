<!DOCTYPE HTML><html><head><meta charset="utf-8" /><link rel="apple-touch-icon" sizes="76x76" href="http://localhost:4000/assets/img/apple-icon.png"><link rel="icon" type="image/png" href="http://localhost:4000/assets/img/favicon.png"><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" /><title>On segmentation of Nerve Structures in Ultrasound Images</title><meta name="description" content="The goal of any image segmentation algorithm is to represent the image in a more meaningful manner and to identify structures and boundaries around them. In its simplest form segmentation is assigning class labels to every pixel in the image. We’ll be segmenting ultrasound images of a region around the neck." /><meta name="author" content="M Reddy" /><meta name="viewport" content='width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0' /><meta name="keywords" content="ai,machine-learning,binary-classification,image-processing,feature-engineering,deep-learning,neural-networks,Arcane, Reveries, Manikanta" /><meta name="msvalidate.01" content="74C35086502D0044BAD289F68057B30D" /><!-- Fonts and icons --><link rel="canonical" href="http://localhost:4000/blog/2016/11/16/ultrasound-nerve-segmentation/" /><link rel="stylesheet" href="https://fonts.googleapis.com/icon?family=Material+Icons" /><link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700" /><link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:400,700|Material+Icons" /><link rel="stylesheet" href="http://localhost:4000/assets/stylesheets/font-awesome.min.css" /><link href="https://fonts.googleapis.com/css?family=Comfortaa" rel="stylesheet"><!-- CSS Files --><link href="http://localhost:4000/assets/stylesheets/application.css" rel="stylesheet" /> <script>var siteUrl = "http://localhost:4000"; var pageUrl = "/blog/2016/11/16/ultrasound-nerve-segmentation/"</script><!-- Open Graph Meta Tags --><meta property="og:url" content="http://localhost:4000/blog/2016/11/16/ultrasound-nerve-segmentation/" /><meta property="og:type" content="article" /><meta property="og:title" content="On segmentation of Nerve Structures in Ultrasound Images" /><meta property="og:description" content="The goal of any image segmentation algorithm is to represent the image in a more meaningful manner and to identify structures and boundaries around them. In its simplest form segmentation is assigning class labels to every pixel in the image. We’ll be segmenting ultrasound images of a region around the neck." /><meta property="og:image" content="http://localhost:4000/assets/posts/2016-11-16-ultrasound-nerve-segmentation/feature.gif" /><meta property="og:image:alt" content="On segmentation of Nerve Structures in Ultrasound Images" /><!-- Facebook Tags --><meta property="fb:app_id" content="1974458362838018" /><!-- Twitter Tags --><meta name="twitter:card" content="summary" /><meta name="twitter:creator" content="@ManikantaReddyD" /><meta name="twitter:site" content="@ManikantaReddyD" /><meta name="twitter:title" content="On segmentation of Nerve Structures in Ultrasound Images" /><meta name="twitter:description" content="The goal of any image segmentation algorithm is to represent the image in a more meaningful manner and to identify structures and boundaries around them. In its simplest form segmentation is assigning class labels to every pixel in the image. We’ll be segmenting ultrasound images of a region around the neck." /><meta name="twitter:image" content="http://localhost:4000/assets/posts/2016-11-16-ultrasound-nerve-segmentation/feature.gif" /><!-- Google Data --> <script type="application/ld+json"> { "@context": "https://schema.org", "@type": "Article", "mainEntityOfPage": { "@type": "WebPage", "@id": "http://localhost:4000/blog/2016/11/16/ultrasound-nerve-segmentation/" }, "headline": "On segmentation of Nerve Structures in Ultrasound Images", "image": [ "http://localhost:4000/assets/posts/2016-11-16-ultrasound-nerve-segmentation/feature.gif" ], "datePublished": "2016-11-16 00:00:00 -0500", "publisher":{ "@type": "Organization", "name": "Arcane Reveries", "logo": { "@type": "ImageObject", "url": "http://localhost:4000/favicon.png" } }, "author": { "@type": "Person", "name": "M Reddy", "sameAs": [ "mailto:manikanta.reddy.d@outlook.com", "https://twitter.com/ManikantaReddyD", "https://facebook.com/Manikanta.D.Reddy", "https://linkedin.com/in/ManikantaDReddy", "https://github.com/ManikantaReddyD", "https://stackoverflow.com/users/6345500/manikanta-reddy-d", "https://www.last.fm/user/ManikantaReddy", "https://soundcloud.com/ManikantaReddyD", "https://open.spotify.com/user/12185478274", "https://www.reddit.com/user/ManikantaReddyD", "skype:manikanta.reddy.d?chat", "" ] }, "description": "The goal of any image segmentation algorithm is to represent the image in a more meaningful manner and to identify structures and boundaries around them. In its simplest form segmentation is assigning class labels to every pixel in the image. We’ll be segmenting ultrasound images of a region around the neck." } </script><body class="index-page"><nav class="navbar navbar-transparent navbar-fixed-top navbar-color-on-scroll"><div class="container"><div class="navbar-header"> <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#navigation-index"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar"></span> <span class="icon-bar"></span> <span class="icon-bar"></span> </button> <a href="http://localhost:4000/"><div class="logo-container" style="padding-left: 12px"><div class="logo"> <img src="http://localhost:4000/assets/img/logo.jpg" alt="Manikanta Reddy D" rel="tooltip" title="Hello! Its Me." data-placement="bottom" data-html="true"></div><div class="logo-brand"> Arcane Reveries</div></div></a></div><div class="collapse navbar-collapse" id="navigation-index"><ul class="nav navbar-nav navbar-right"><li> <a href="http://localhost:4000/about"> <i class="material-icons">account_circle</i> About </a><li> <a href="http://localhost:4000/blog"> <i class="material-icons">dashboard</i> Posts </a><li> <a href="http://localhost:4000/tags"> <i class="fa fa-tag"></i> Tags </a></ul></div></div></nav><div class="wrapper"><div class="header header-filter" ><div class="container" style=" background-image: url('http://localhost:4000/assets/posts/2016-11-16-ultrasound-nerve-segmentation/feature.gif'); width:100%; background-position: center; " ><div class="row" style=" background: rgba(39,62,84,0.82); overflow: hidden; height: 100%; z-index: 500; " ><div class="col-md-8 col-md-offset-2"><div class="brand"><h1>On segmentation of Nerve Structures in Ultrasound Images</h1><h3></h3></div></div></div></div></div><div class="main"><div class="section"><div class="container"><div class="col-md-10 col-lg-offset-1" style="font-family: 'Comfortaa'"><blockquote><p>The goal of any image segmentation algorithm is to represent the image in a more meaningful manner and to identify structures and boundaries around them. In its simplest form segmentation is assigning class labels to every pixel in the image. We’ll be segmenting ultrasound images of a region around the neck.</blockquote><div class="row"><p class="col-md-6">16 November 2016<p class="col-md-6">Reading time ~12 minutes</div><div style="overflow: hidden;"><div class="tags" style="white-space: nowrap; overflow-x: auto; overflow-y:hidden; width: 100%; height: 50px"> <button class="btn btn-simple btn-primary" style="padding: 5px;"><a href="http://localhost:4000/tags/ai" class="">ai</a></button> <button class="btn btn-simple btn-primary" style="padding: 5px;"><a href="http://localhost:4000/tags/machine-learning" class="">machine-learning</a></button> <button class="btn btn-simple btn-primary" style="padding: 5px;"><a href="http://localhost:4000/tags/binary-classification" class="">binary-classification</a></button> <button class="btn btn-simple btn-primary" style="padding: 5px;"><a href="http://localhost:4000/tags/image-processing" class="">image-processing</a></button> <button class="btn btn-simple btn-primary" style="padding: 5px;"><a href="http://localhost:4000/tags/feature-engineering" class="">feature-engineering</a></button> <button class="btn btn-simple btn-primary" style="padding: 5px;"><a href="http://localhost:4000/tags/deep-learning" class="">deep-learning</a></button> <button class="btn btn-simple btn-primary" style="padding: 5px;"><a href="http://localhost:4000/tags/neural-networks" class="">neural-networks</a></button></div></div><hr /><h1 id="overview">Overview</h1><p>Pain is one of the most important aspects, that has to be properly managed, for easing the suffering of a patient and improve their quality of life. Pain management is typically done through the administration of narcotics, which also bring with them other unwanted effects. On the other hand introducing pain management catheters<sup id="fnref:1"><a href="#fn:1" class="footnote">1</a></sup>, that block or mitigate the pain at source, reduce dependency on the narcotics and speed up the recovery process. We will discuss the problem of identifying nerve structures in the region around neck so that the indwelling catheters are rightly placed and provide a pain free future.<p>Accurately identifying the nerve structures, also called the <em>Brachial Plexus</em><sup id="fnref:2"><a href="#fn:2" class="footnote">2</a></sup>, in the ultrasound images of the region around the neck can be modeled as an image segmentation problem. By placing the nerve structure in one class and the rest in the contrary, the problem is further simplified to a binary classification problem. Many methods exist to perform binary image segmentation, ranging from simple threshold based approaches to ones that use very deep neural networks. We will be discussing a method based on proposals by overlapping windows and the other, based on the U-net architecture<sup id="fnref:3"><a href="#fn:3" class="footnote">3</a></sup>. We will also look into how the choice of architecture of the neural net, makes the learning different in the deeper layers.<h2 id="problem">Problem</h2><p>The region of our interest is a collection of nerve structures called <em>Brachial Plexus</em>, which extends over the spinal cord, through the neck and into the armpit.<p><img src="http://localhost:4000/assets/posts/2016-11-16-ultrasound-nerve-segmentation/Brachial_Plexus.jpg" alt="Brachial Plexus" /> <center>Brachial Plexus</center><p>Identifying the accurate location of this nerve structure is a critical step in inserting the patients pain management catheter. The problem is posted as a challenge on <em>Kaggle</em><sup id="fnref:4"><a href="#fn:4" class="footnote">4</a></sup> to build a model that can locate the plexus in a dataset of ultrasound images of the neck.<h1 id="data">Data</h1><p>The dataset provided consists of large training set of ultrasound images, in which the nerve structure is manually annotated by trained experts. The experts make their mark on the basis of their confidence of existence of BP in the image.<p>Given below are two images, one is the ultra sound image itself and the other is the human annotation.<div class="row"><div class="col-md-6"> <img src="http://localhost:4000/assets/posts/2016-11-16-ultrasound-nerve-segmentation/47_117.png" width="100%" /> Image: 47_117</div><div class="col-md-6"> <img src="http://localhost:4000/assets/posts/2016-11-16-ultrasound-nerve-segmentation/47_117_mask.png" width="100%" object-fit="contain" /> Image: 47_117_mask</div></div><center> Image and its corresponding mask </center><p>The white region represents our region of interest. The model which we will build has to take the gray scale ultrasound image on the left as input and output a binary mask, similar to the binary image on the right. It has to be noted that the output is not gray scale. The intensities will be exclusively either 0 or 255.<p><img src="https://www.kaggle.io/svf/247972/dd3372f74b069aa9770169ba5beb167d/patient-41.gif" alt="&lt;span data-label=&quot;fig:overlayimage&quot;&gt;&lt;/span&gt;" /> <center>Image with overlay. Notice the red boundary over the region of interest<br />Source: <a href="https://www.kaggle.com/chefele/ultrasound-nerve-segmentation/animated-images-with-outlined-nerve-area/code">Animated Images with Outlined Nerve Area </a></center><p>Priori analysis of the data reveals that about 60% of the images do not posses a mask, implying that they have no BP landmark. Hence we might not worry about the statistical bias in the supply of data.<p>Other notable key points about the dataset,<ul><li><p>Image size: 580x420 pixels<li><p>5635 training images<li><p>5508 testing images<li><p>Noise and other artifacts present<li><p>Repetition of the images</ul><h2 id="inconsistencies-in-the-data-set">Inconsistencies in the Data-set</h2><p>There are potential mistakes in the ground truth of the data. By this we mean that there are images, which are very similar to each other yet have differing masks<sup id="fnref:5"><a href="#fn:5" class="footnote">5</a></sup>.<p>One of the images has a mask while the other doesn’t and due to the nature of loss function we use for this problem, false positives and false negatives are penalized heavily.<p>The occurrence of such contrary images is highly likely in the test data-set provided. Coupled with the loss function, this particular empirical observation sets an upper bound on the results.<p>Another aspect of the image generation is the way they are produced. It seems that the images are frames of the ultrasound video feed, due to which we encounter multiple images which are actually the same frame. <sup id="fnref:6"><a href="#fn:6" class="footnote">6</a></sup><h2 id="dice-coefficient">Dice Coefficient</h2><p>It has been suggestedthat <em>Dice coefficient</em> be used as an error measure. Given two sets $A$ and $B$, dice index of them is, <script type="math/tex; mode=display">Q = \frac{|A\cap B|}{|A| + |B|}</script><p>This function penalizes any kind of wrong predictions. Just capturing the where and what is not enough, the prediction and truth have to be highly correlated. It is necessary that the binary outputs look as good as the real ones.<h1 id="data-pre-processing">Data Pre Processing</h1><p>In order to minimize the inherent inconsistencies, we pre process the whole of the data-set.<h2 id="image-down-scaling">Image Down Scaling</h2><p>The images have been scaled down from 580x420 to 128x128. Other scales haven’t been tested and could perform better. The reason to choose this scale is largely for programmatic ease and to avoid memory overflows in the system that’s been used.<h2 id="removal-of-inconsistent-data">Removal of Inconsistent data</h2><p>In order to minimize contradictory data, we will remove all such pairs of images from data-set, which although are morphologically similar posses different label masks.<p>The closeness of two images was determined using cosine distance over block histograms of the images. Instead of dropping such images, propagating the masks over to the empty one, was also tried, but this didn’t offer any convincing spike in the prediction power and rather degraded performance.<h2 id="generating-more-data">Generating More Data</h2><p>One of the main advantages of working on medical images is that the objects of interest in the images are organic. They are indifferent to distortions and warping. Therefore we can freely transform the images by slightly changing them (Apply small affine transformations) to generate a completely different image that although different, can still be considered as a valid image for training. This compensates for the images we dropped previously.<h1 id="sliding-window-proposals">Sliding Window Proposals</h1><p>This method is based on the idea that, the label a pixel will take depends on who its neighbors are and what labels they are taking. In order to encapsulate the information about what and how the neighbors of a pixel are doing, we consider a window around the pixel and map the pixels to a functional value and then use this functional value as a measure of confidence the window has, in the pixel, towards a particular class.<div class="row"><div class="col-md-6"> <img width="100%" src="http://localhost:4000/assets/posts/2016-11-16-ultrasound-nerve-segmentation/Selection_001.png" /></div><div class="col-md-6"> <img width="100%" src="http://localhost:4000/assets/posts/2016-11-16-ultrasound-nerve-segmentation/Selection_002.png" /></div></div><p>In the figures above, both the images have the window overlay, in white, at the same location. We call such windows <em>corresponding</em> windows. They are capturing information from the same region, albeit in a different plane.<h2 id="sliding-windows">Sliding Windows</h2><p>We’ll consider $w$ as the window in the ultrasound image, or $x$, and $w_{mask}$ as the corresponding window in the mask image, or $y$. Every pixel in the image has some intensity value given by some intensity function $I:p_i \rightarrow (0,255)$. The window($w$) will contain some finite number of pixels and can be written as a set of all the pixels in the window as <script type="math/tex">w = \{p_i / p_i \in w\}</script> or in a vector form as <script type="math/tex">w = (I(p_1), I(p_2), ..., I(p_i), ...).</script><p>Henceforth, $w_j$ will be the window in mask and ${w_{mask}}_j$ is the corresponding window in the mask and $W$ be range space of all windows. We can see that all windows over an image will therefore be unique sets of pixels. These sets may overlap, that is, a pixel can be present in multiple windows.<p>In our method, we try to build two functions $\Gamma: W\rightarrow [0,1]$ and $\Theta: W\rightarrow [0,1]$ such that <script type="math/tex; mode=display">\Gamma(w_j) = \Theta({w_{mask}}_j)</script><p>For our problem we define $\Theta$ as the mean of binary values of pixels in the window. <script type="math/tex; mode=display">\Theta({w_{mask}}_j) = \frac{\sum\limits_{p_i \in {w_{mask}}_j}^n I(p_i)}{n}</script><p>We approximate $\Gamma$ as a logistic regression model and learn it through training which can be setup easily by dividing the images into sliding boxes with overlapping (some stride length). These boxes can be considered as windows.<p>$\Theta$ in a practical sense gives us an idea of how much interesting region it contains. $\Gamma$ then tries to figure out what feature of the original pixels generates such interesting region. $\Gamma(w)$ is the probability that the window has some mask.<p>By the end of this we land up with $\Gamma$, a function capable of predicting how much of the window belongs to the region of interest.<h2 id="the-proposals">The Proposals</h2><p>If we are able to somehow predict the class of a pixel our problem is done. We have already devised a way of relating the windows to region of interests. We now try to devise a way to use the information about windows for pixels.<p>Every pixel in the image will belong to some finite number of windows, owing to the way how the windows are built. Every such window will capture some neighborhood of our pixel. Thus windows can be considered good representatives of the neighborhoods of the pixel. Any operation the window is an operation on the neighborhood.<p>If a pixel neighborhood holds some property, it is very <em>likely</em> that the property will hold for the pixel too. If windows have some probability of having some mask, so does the pixel. A window makes a proposal for a pixel to be a pixel in region of interest.<p>Since a pixel can belong to multiple windows, multiple neighborhoods have an affect on it simultaneously. We now make an important assumption that multiple neighborhoods affect the pixel <em>independently</em>.<p>Thus using the above arguments, we define the probability$(P(p_i))$ that a pixel $p_i$ will be a pixel in region of interest as <script type="math/tex; mode=display">P(p_i) = \prod\limits_{p_i \in w_j} \Gamma(w_j)</script><p>Using this probability as a measure and by choosing appropriate thresholds, we can generate masks for any ultrasound image.<h1 id="neural-networks">Neural Networks</h1><h2 id="encoder-decoder">Encoder-Decoder</h2><p>We are motivated by a simple desire to preserve spatial locality of the region of interest in the context of the entire image. We built a simple Encoder-Decoder network to do the same.<p>We hope that the network learns a representation just enough to distinguish pixels of the nerve structure from the rest.<p><img src="http://localhost:4000/assets/posts/2016-11-16-ultrasound-nerve-segmentation/Autoencoder_structure.png" alt="Encoder Decoder&lt;span data-label=&quot;fig:autoencoder&quot;&gt;&lt;/span&gt;" /> The network built was 4 pool layers deep with 3 convolutions in each layer. There is a simple contracting path and then a expanding path, that embed the information present in the entire image into lower dimensional vectors. The size of this vector is halved at every pool layer.<p>Autoencoders<sup id="fnref:9"><a href="#fn:9" class="footnote">7</a></sup> can capture the context of the image in a very good manner in the representation it learns, but doesn’t perform well when trying to learn the spatial localization present in the image.<p>In other words, this network captures the <em>’what’</em> in the image but not the <em>’where’</em> that particular answer to ’what’ is present.<h2 id="u-net">U-net</h2><p>We understand that the encoder decoder fails at one particular learning problem, capturing the spatial localization. In order to achieve this, all we have to do is provide the spatial information as well.<p>We do so by combining the high resolution features from the contracting path with the up-sampling features in the expanding path.<p><img src="http://localhost:4000/assets/posts/2016-11-16-ultrasound-nerve-segmentation/unet.png" alt="U-net&lt;span data-label=&quot;fig:unet&quot;&gt;&lt;/span&gt;" /> <center> U-net architecture</center><p>U-net<sup id="fnref:3:1"><a href="#fn:3" class="footnote">3</a></sup> has two parts. i) Encoder and ii) Decoder. The first part is simple convolution network that embeds the image into lower dimensional vectors. The second part is also a simple upsampling network, but, with a change.<p>At every pooling step in the encoder network, we collect the vectors and feed them (concatenate with) the upsampled vectors from the previously layer in the decoder network. (Note the <em>Concatenate local features</em> step in the image.<p>We are forcing the network to give emphasis on the high resolution features that are lost during the encoding step. During the contraction phase, the <em>’what’</em> in the image is highly consolidated, <em>’where’</em> is lost. The concatenation step, forces the network to relearn the <em>’where’</em>, which it ignored in the contraction.<p>The final output will thus contain both the context and localization information in it.<h2 id="dreams">Dreams</h2><p>Dreams<sup id="fnref:8"><a href="#fn:8" class="footnote">8</a></sup> are a good way to visualize what the features, the filters are learning. They represent what kind of input ,<em>excites</em>, the filters.<div class="row"><div class="col-md-6" padding="0" width="100%"> <img src="http://localhost:4000/assets/posts/2016-11-16-ultrasound-nerve-segmentation/autoencoder_dream.png" /> Auto Encoder's Dream</div><div class="col-md-6"> <img src="http://localhost:4000/assets/posts/2016-11-16-ultrasound-nerve-segmentation/unet_dream.png" /> U-net's dream</div></div><center> Notice the fibrous structures in U-net's dream. </center><p>As you can clearly see, U-net responds well to images with Brachial Plexus, something that autoencoder failed. Autoencoder would respond very well to ultrasound images of the neck, but can’t find where exactly this nerve structure is present in it.<h1 id="post-processing">Post Processing</h1><p>The output generated by any of the methods discussed is just a probability map of every pixel’s likeliness to be in the region of interest. But as the problem demands in order to score, we should have output masks that would be as close to human like annotations. In order to convert this probability map into an annotation like image we employ different techniques.<h2 id="thresholding">Thresholding</h2><p>We could simply set a threshold over the probability and generate binary outputs. But this solution, puts forward a serious problem.<p><img src="http://localhost:4000/assets/posts/2016-11-16-ultrasound-nerve-segmentation/holes.png" alt="" /> <center>Discontinuity in mask generation</center><p>On what basis do we chose the threshold? Certainly it is not the visual appeal. We should also not try to train a model to learn the threshold, because if something like that were possible, it would have been picked up by the model itself (at least in the neural networks).<p>The second problem is related to the discontinuous maps it can create. The threshold can be set in such a way that a certain region in the middle can be labeled the other way. The problem of undesired holes can be solved by applying a <em>morphological</em> closing method, but still we would not have outputs that look human like.<h2 id="pca-based-cleaning">PCA based cleaning</h2><p>The desire to generate human-like annotations, points us to the question, what makes a mask human-like? Can we learn this too? We have many images that can teach us to generate such masks.<p>In order to learn such features, we train a PCA model on all training masks, with region of interest. The PCA model will then give us some <em>Eigen Masks</em>, or the inner features of drawing an annotation. Using a linear combination of these eigen masks, we should be in principle able to generate masks, that look like human annotated masks.<div class="row"><div class="col-md-6"> <img src="http://localhost:4000/assets/posts/2016-11-16-ultrasound-nerve-segmentation/probability_mask.png" /> Probability Mask</div><div class="col-md-6"> <img src="http://localhost:4000/assets/posts/2016-11-16-ultrasound-nerve-segmentation/pca_cleaned.png" /> PCA transform of the Mask</div></div><center> PCA transformation of probability masks </center><h1 id="results">Results</h1><p>The results are computed by taking the mean of Dice score of the predicted masks with the annotated masks. The scores shown here are generated by kaggle. <center> Average Dice Coefficients</center><table><thead><tr><th>Model<th>Best Dice Score<tbody><tr><td>Blank submission<td>0.53449<tr><td>Proposal Based<td>0.56527<tr><td>Autoencoder<td>0.62322<tr><td>U-net (without PCA)<td>0.6689<tr><td>U-net (with PCA)<td>0.68719</table><p><br /><p>A blank submission, itself gives us a score of $0.53$ indicating that about half of the images do not contain any mask.<p>The proposal based method used windows of size $30x30$ at a stride length of $10$. Experimentation with respect to window size and length is yet to be done.<p>The auto-encoder based method was a huge jump from previous solutions, as it was capable enough to capture the nerve structures as we have seen.<p>Introducing extra connections in the encoder-decoder network to build the U-net brought the score up by $4\%$, indicating a strong learning of localization information.<p>Post processing using PCA cleaning definitely brought a final bump in the score by about $2\%$. A much better post processing could also work.<h1 id="references">References</h1><div class="footnotes"><ol><li id="fn:1"><p><a href="https://www.med.umich.edu/1libr/Anesthesiology/PeripheralNerveCatheterWhatIstIt.pdf">Pain Management Catheters</a>&nbsp;<a href="#fnref:1" class="reversefootnote">&#8617;</a><li id="fn:2"><p><a href="https://en.wikipedia.org/wiki/Brachial_plexus">Wikipedia: Brachial Plexus</a>&nbsp;<a href="#fnref:2" class="reversefootnote">&#8617;</a><li id="fn:3"><p><a href="https://arxiv.org/pdf/1505.04597.pdf">U-net: Convolutional networks for biomedical image</a>&nbsp;<a href="#fnref:3" class="reversefootnote">&#8617;</a>&nbsp;<a href="#fnref:3:1" class="reversefootnote">&#8617;<sup>2</sup></a><li id="fn:4"><p><a href="https://www.kaggle.com/c/ultrasound-nerve-segmentation/">Kaggle: Ultrasound Nerve Segmentation</a>&nbsp;<a href="#fnref:4" class="reversefootnote">&#8617;</a><li id="fn:5"><p><a href="https://www.kaggle.com/agalea91/ultrasound-nerve-segmentation/mislabeled-training-images/run/310043">Mislabeled Training Images</a>&nbsp;<a href="#fnref:5" class="reversefootnote">&#8617;</a><li id="fn:6"><p><a href="https://www.kaggle.com/c/ultrasound-nerve-segmentation/data">Data Description</a>&nbsp;<a href="#fnref:6" class="reversefootnote">&#8617;</a><li id="fn:9"><p><a href="https://www.cs.toronto.edu/~hinton/science.pdf">Hinton et al, 2006</a>&nbsp;<a href="#fnref:9" class="reversefootnote">&#8617;</a><li id="fn:8"><p><a href="https://blog.keras.io/category/demo.html">Deep Dreams</a>&nbsp;<a href="#fnref:8" class="reversefootnote">&#8617;</a></ol></div><hr /><div class="row comments animated fadeInUp"><h3>Comments</h3><button id="show-comments" class="btn btn-primary">Load comments</button><div id="comments-spinner" style="display: none;"><!--<div class="spinner"><div class="rect1"></div><div class="rect2"></div><div class="rect3"></div><div class="rect4"></div><div class="rect5"></div></div>--><div style="text-align: center;"> <svg width="33%" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 100 100" preserveAspectRatio="xMidYMid" class="lds-infinity"><path fill="none" ng-attr-stroke="" ng-attr-stroke-width="" ng-attr-stroke-dasharray="" d="M24.3,30C11.4,30,5,43.3,5,50s6.4,20,19.3,20c19.3,0,32.1-40,51.4-40 C88.6,30,95,43.3,95,50s-6.4,20-19.3,20C56.4,70,43.6,30,24.3,30z" stroke="#03a9f4" stroke-width="1" stroke-dasharray="6.414723205566406 6.414723205566406"> <animate attributeName="stroke-dashoffset" calcMode="linear" values="0;256.58892822265625" keyTimes="0;1" dur="5" begin="0s" repeatCount="indefinite"></animate> </path> </svg></div></div><div id="comments" style="display: none;"><ul class="nav nav-pills"><li class="active"><a data-toggle="tab" href="#fb_comments">Facebook</a><li><a data-toggle="tab" href="#disqus_comments">Disqus</a></ul><hr /><div class="tab-content"><div id="fb_comments" class="tab-pane active"><div class="fb-comments" data-href="http://localhost:4000/blog/2016/11/16/ultrasound-nerve-segmentation/" data-colorscheme="light" data-num-posts="4" data-width="100%"></div></div><div id="disqus_comments" class="tab-pane"><div id="disqus_thread"></div></div></div></div></div><h2 style="text-align: center;">More Posts</h2><div class="row"><div class="col-md-6"><div class="card card-raised"><div class="card-image" style="background-image: url('/assets/posts/2017-04-21-possible-worlds/feature.jpg'); "></div><div class="content"> <a href="http://localhost:4000/blog/2017/04/21/possible-worlds/"><h3>Possibility of Possible Worlds</h3></a><div class="row"><p class="col-md-6">21 April 2017<p class="col-md-6">Reading time ~12 minutes</div><p> We humans have an innate tendency to question everything. We are always drowned in an infinite ocean of provocative thoughts, but a small stream keeps on asking, why is everything the way it is? W...</div><div class="tags content-bottom"> <button class="btn btn-simple btn-primary" style="padding: 5px;"><a href="http://localhost:4000/tags/philosophy" class="">philosophy</a></button> <button class="btn btn-simple btn-primary" style="padding: 5px;"><a href="http://localhost:4000/tags/logic" class="">logic</a></button> <button class="btn btn-simple btn-primary" style="padding: 5px;"><a href="http://localhost:4000/tags/non-classical-logic" class="">non-classical-logic</a></button></div></div></div><div class="col-md-6"><div class="card card-raised"><div class="card-image" style="background-image: url('/assets/posts/2016-10-27-inversion-of-control/feature.png'); "></div><div class="content"> <a href="http://localhost:4000/blog/2016/10/27/inversion-of-control/"><h3>Inversion of Control</h3></a><div class="row"><p class="col-md-6">27 October 2016<p class="col-md-6">Reading time ~5 minutes</div><p>A sample implementation of the Inversion of Control principle with a comparative analysis of the approach with a traditional formulation of the same problem.</div><div class="tags content-bottom"> <button class="btn btn-simple btn-primary" style="padding: 5px;"><a href="http://localhost:4000/tags/software-engineering" class="">software-engineering</a></button> <button class="btn btn-simple btn-primary" style="padding: 5px;"><a href="http://localhost:4000/tags/dependency-injection" class="">dependency-injection</a></button></div></div></div></div></div></div></div></div></div><!-- Core JS Files --> <script src="http://localhost:4000/assets/javascripts/vendors/jquery.min.js" type="text/javascript"></script> <script src="http://localhost:4000/assets/javascripts/vendors/material-kit/bootstrap.min.js" type="text/javascript"></script> <script src="http://localhost:4000/assets/javascripts/vendors/material-kit/material.min.js"></script><!-- <script src="http://localhost:4000/assets/javascripts/vendors/material-kit/nouislider.min.js" type="text/javascript"></script> --> <script src="http://localhost:4000/assets/javascripts/vendors/material-kit/bootstrap-datepicker.js" type="text/javascript"></script> <script src="http://localhost:4000/assets/javascripts/vendors/material-kit/material-kit.js" type="text/javascript"></script> <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML"></script> <script type="text/javascript"> MathJax.Hub.Config({ tex2jax: { inlineMath: [ ['$','$'], ['\\(','\\)'] ] } }); </script><!-- <script type="text/javascript"> $().ready(function () { materialKit.initSliders(); window_width = $(window).width(); if (window_width >= 992) { big_image = $('.wrapper > .header'); $(window).on('scroll', materialKitDemo.checkScrollForParallax); } }); </script> --> <script type="text/javascript"> var load_disqus = function(){ var disqus_shortname = 'manikantadreddy'; (function() { var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true; dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js'; (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq); })(); (function () { var s = document.createElement('script'); s.async = true; s.type = 'text/javascript'; s.src = '//' + disqus_shortname + '.disqus.com/count.js'; (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s); }()); }; </script> <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript><div id="fb-root"></div><script> var load_facebook = function(){ (function (d, s, id) { var js, fjs = d.getElementsByTagName(s)[0]; if (d.getElementById(id)) return; js = d.createElement(s); js.id = id; js.src = "//connect.facebook.net/en_US/sdk.js#xfbml=1&version=v2.5&appId=1166944040038372"; fjs.parentNode.insertBefore(js, fjs); }(document, 'script', 'facebook-jssdk')); }; </script> <script src="http://localhost:4000/assets/javascripts/comments.js"></script> <script> $(document).on('click', 'a[href^="#"]', function (event) { event.preventDefault(); $('html, body').animate({ scrollTop: $($.attr(this, 'href')).offset().top -150 }, 500); }); </script>
    