<!DOCTYPE HTML><html><head><meta charset="utf-8" /><link rel="apple-touch-icon" sizes="76x76" href="http://localhost:4000/assets/img/apple-icon.png"><link rel="icon" type="image/png" href="http://localhost:4000/assets/img/favicon.png"><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" /><title>A survey on Human Sex Determination methods</title><meta name="description" content="Identifying the sex of an individual is a challenging problem in Computer Vision. In the recent years various methods have surfaced to solve this classification problem and get closer to achieve better human machine interaction. In this survey we study a number of such algorithms which classify human subjects into male and female classes. This task is done by looking at a number of features, like face, body and motion." /><meta name="author" content="M Reddy" /><meta name="viewport" content='width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0' /><meta name="keywords" content="ai,machine-learning,binary-classification,image-processing,sex,feature-engineering,Arcane, Reveries, M" /><meta name="msvalidate.01" content="" /><!-- Fonts and icons --><link rel="canonical" href="http://localhost:4000/blog/2016/04/09/can-we-discriminate/" /><link rel="stylesheet" href="https://fonts.googleapis.com/icon?family=Material+Icons" /><link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700" /><link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:400,700|Material+Icons" /><link rel="stylesheet" href="http://localhost:4000/assets/stylesheets/font-awesome.min.css" /><link href="https://fonts.googleapis.com/css?family=Comfortaa" rel="stylesheet"><!-- CSS Files --><link href="http://localhost:4000/assets/stylesheets/application.css" rel="stylesheet" /> <script>var siteUrl = "http://localhost:4000"; var pageUrl = "/blog/2016/04/09/can-we-discriminate/"</script><!-- Open Graph Meta Tags --><meta property="og:url" content="http://localhost:4000/blog/2016/04/09/can-we-discriminate/" /><meta property="og:type" content="article" /><meta property="og:title" content="A survey on Human Sex Determination methods" /><meta property="og:description" content="Identifying the sex of an individual is a challenging problem in Computer Vision. In the recent years various methods have surfaced to solve this classification problem and get closer to achieve better human machine interaction. In this survey we study a number of such algorithms which classify human subjects into male and female classes. This task is done by looking at a number of features, like face, body and motion." /><meta property="og:image" content="http://localhost:4000/assets/posts/2016-04-09-can-we-discriminate/feature.gif" /><meta property="og:image:alt" content="A survey on Human Sex Determination methods" /><!-- Facebook Tags --><!-- Twitter Tags --><meta name="twitter:card" content="summary" /><meta name="twitter:creator" content="@" /><meta name="twitter:site" content="@" /><meta name="twitter:title" content="A survey on Human Sex Determination methods" /><meta name="twitter:description" content="Identifying the sex of an individual is a challenging problem in Computer Vision. In the recent years various methods have surfaced to solve this classification problem and get closer to achieve better human machine interaction. In this survey we study a number of such algorithms which classify human subjects into male and female classes. This task is done by looking at a number of features, like face, body and motion." /><meta name="twitter:image" content="http://localhost:4000/assets/posts/2016-04-09-can-we-discriminate/feature.gif" /><!-- Google Data --> <script type="application/ld+json"> { "@context": "https://schema.org", "@type": "Article", "mainEntityOfPage": { "@type": "WebPage", "@id": "http://localhost:4000/blog/2016/04/09/can-we-discriminate/" }, "headline": "A survey on Human Sex Determination methods", "image": [ "http://localhost:4000/assets/posts/2016-04-09-can-we-discriminate/feature.gif" ], "datePublished": "2016-04-09 00:00:00 -0400", "publisher":{ "@type": "Organization", "name": "Arcane Reveries", "logo": { "@type": "ImageObject", "url": "http://localhost:4000/favicon.png" } }, "author": { "@type": "Person", "name": "M Reddy", "sameAs": [ "" ] }, "description": "Identifying the sex of an individual is a challenging problem in Computer Vision. In the recent years various methods have surfaced to solve this classification problem and get closer to achieve better human machine interaction. In this survey we study a number of such algorithms which classify human subjects into male and female classes. This task is done by looking at a number of features, like face, body and motion." } </script><body class="index-page"><nav class="navbar navbar-transparent navbar-fixed-top navbar-color-on-scroll"><div class="container"><div class="navbar-header"> <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#navigation-index"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar"></span> <span class="icon-bar"></span> <span class="icon-bar"></span> </button> <a href="http://localhost:4000/"><div class="logo-container" style="padding-left: 12px"><div class="logo"> <img src="http://localhost:4000/assets/img/logo.jpg" alt="Manikanta Reddy D" rel="tooltip" title="Hello! Its Me." data-placement="bottom" data-html="true"></div><div class="logo-brand"> Confabs</div></div></a></div><div class="collapse navbar-collapse" id="navigation-index"><ul class="nav navbar-nav navbar-right"><li> <a href="http://localhost:4000/about"> <i class="material-icons">account_circle</i> About </a><li> <a href="http://localhost:4000/blog"> <i class="material-icons">dashboard</i> Posts </a><li> <a href="http://localhost:4000/tags"> <i class="fa fa-tag"></i> Tags </a></ul></div></div></nav><div class="wrapper"><div class="header header-filter" ><div class="container" style=" background-image: url('http://localhost:4000/assets/posts/2016-04-09-can-we-discriminate/feature.gif'); width:100%; background-position: center; " ><div class="row" style=" background: rgba(39,62,84,0.82); overflow: hidden; height: 100%; z-index: 500; " ><div class="col-md-8 col-md-offset-2"><div class="brand"><h1>A survey on Human Sex Determination methods</h1><h3></h3></div></div></div></div></div><div class="main"><div class="section"><div class="container"><div class="col-md-10 col-lg-offset-1" style="font-family: 'Comfortaa'"><blockquote><p>Identifying the sex of an individual is a challenging problem in Computer Vision. In the recent years various methods have surfaced to solve this classification problem and get closer to achieve better human machine interaction. In this survey we study a number of such algorithms which classify human subjects into male and female classes. This task is done by looking at a number of features, like face, body and motion.</blockquote><div class="row"><p class="col-md-6">09 April 2016<p class="col-md-6">Reading time ~8 minutes</div><div style="overflow: hidden;"><div class="tags" style="white-space: nowrap; overflow-x: auto; overflow-y:hidden; width: 100%; height: 50px"> <button class="btn btn-simple btn-primary" style="padding: 5px;"><a href="http://localhost:4000/tags/ai" class="">ai</a></button> <button class="btn btn-simple btn-primary" style="padding: 5px;"><a href="http://localhost:4000/tags/machine-learning" class="">machine-learning</a></button> <button class="btn btn-simple btn-primary" style="padding: 5px;"><a href="http://localhost:4000/tags/binary-classification" class="">binary-classification</a></button> <button class="btn btn-simple btn-primary" style="padding: 5px;"><a href="http://localhost:4000/tags/image-processing" class="">image-processing</a></button> <button class="btn btn-simple btn-primary" style="padding: 5px;"><a href="http://localhost:4000/tags/sex" class="">sex</a></button> <button class="btn btn-simple btn-primary" style="padding: 5px;"><a href="http://localhost:4000/tags/feature-engineering" class="">feature-engineering</a></button></div></div><hr /><hr /><h1 id="introduction">Introduction</h1><p>We provide a survey of human sex recognition using computer vision techniques. We review multiple methods the exploit information from face and whole body. Identifying demographic attributes is a key point of machine-human interaction. While humans can do this task very easily it is a challenging task for computer vision.<p>In general, we breakdown the problem into multiple phases. Object detection, preprocessing, feature extraction and classification. All these sub problems depend on the choice of feature we wish to encode and use. Once the object of interest is detected we then process it to extract features and their corresponding descriptors. We then use these descriptors to train a classifier. <sup id="fnref:ng2012vision"><a href="#fn:ng2012vision" class="footnote">1</a></sup><h1 id="face-based-methods">Face Based Methods</h1><p>While using faces as our objects, we realize that the face region could also include features from hair and neck. A human also exhibits variation which make it hard for us to encode them using raw pixel values. The features also depend on race, age, expression and accessories worn. <sup id="fnref:viola2001rapid"><a href="#fn:viola2001rapid" class="footnote">2</a></sup> <sup id="fnref:viola2004robust"><a href="#fn:viola2004robust" class="footnote">3</a></sup><h2 id="dimensionality-reduction-using-pca">Dimensionality Reduction using PCA</h2><p>To extract feature vector from our facial image, we used the technique of dimensionality reduction using PCA<sup id="fnref:jolliffe2002principal"><a href="#fn:jolliffe2002principal" class="footnote">4</a></sup>. PCA is a transformation scheme which transforms the data to a new coordinate system. The axes in this new coordinate system are found by finding the directions of maximum variance in our data. Direction of maximum variance is found by calculating the Eigenvector of covariance matrix corresponding to large Eigen values.<p>Around $95\%$ of total variance was explained by 80 dimensions. Following Eigenfaces were obtained after dimensionality reduction of image. We used these Eigenfaces<sup id="fnref:turk1991face"><a href="#fn:turk1991face" class="footnote">5</a></sup> as input to our classifier for classification to male and female.<p><img src="http://localhost:4000/assets/posts/2016-04-09-can-we-discriminate/figure_1.png" alt="" /><p>We tried Linear Support Vector classifier and Decision tree based classification. Following results were obtained.<div class="row"><div class="col-md-6"> <img src="http://localhost:4000/assets/posts/2016-04-09-can-we-discriminate/2.png" /></div><div class="col-md-6"> <img src="http://localhost:4000/assets/posts/2016-04-09-can-we-discriminate/3.png" /></div></div><p>The classification accuracy seems to be pretty good around  98%. As expected, SVM classifier performs better than decision forests.<h2 id="data-set-used">Data Set Used</h2><div class="row"><div class="col-md-6"> Nottingham Dataset[^nottinghamdataset] <img src="http://localhost:4000/assets/posts/2016-04-09-can-we-discriminate/21.png" /></div><div class="col-md-6"> Faces94[^faces94dataset] <img src="http://localhost:4000/assets/posts/2016-04-09-can-we-discriminate/20.png" /></div></div><h2 id="lowe-key-point-descriptors">Lowe Key Point Descriptors</h2><p>We tried to encapsulate the key points of the image by using a Lowe Descriptors in an orderly fashion. We divided the image into grids and generate Lowe descriptors for the intersection points. Lowe descriptor<sup id="fnref:lowe2004distinctive"><a href="#fn:lowe2004distinctive" class="footnote">6</a></sup> is essentially a histogram of gradient map of the neighborhood of the descriptor.<p><img src="http://localhost:4000/assets/posts/2016-04-09-can-we-discriminate/22.png" alt="image" /> We generated key point descriptors in a grid wise manner for every person and performed a classification based on them.<p>We then flattened all the key point descriptors in an order, row by row and used it as a descriptor for the image.<p>We then used multiple classifying algorithms to identify sex. Here are the results of our experiment.<h3 id="accuracies">Accuracies</h3><div width="50%"><table><thead><tr><th>Classifier<th>Lowe Descriptor<tbody><tr><td>LinearSVC<td><span class="math inline">0.611 ± 0.002</span><tr><td>Random Forest<td><span class="math inline">0.600 ± 0.002</span><tr><td>Perceptron<td><span class="math inline">0.628 ± 0.090</span><tr><td>Adaboost<td><span class="math inline">0.621 ± 0.005</span></table></div><p>We realize at this point that using Lowe descriptors wasn’t a very good idea as the orderly way we encoded the vectors didn’t actually capture the features (Spatial Description in the form of geometric features).\ This might be the reason for very low performance of this method.<h2 id="online-port">Online Port</h2><p><img src="http://localhost:4000/assets/posts/2016-04-09-can-we-discriminate/23.png" /><h2 id="demonstration">Demonstration</h2><p>We ported the algorithm based on facial features to work on the IIT Kanpur Pedestrian Data set and the results were visually pleasing. The blue box around a person’s face indicates that his sex is male. As we have haar cascades for frontal face, all the faces in the frame are not detected and hence are not classified.<p><img src="http://localhost:4000/assets/posts/2016-04-09-can-we-discriminate/41.png" alt="image" /><h2 id="caveats">Caveats</h2><p>The current implementation is not very robust to uncontrolled environmental conditions, as we rely on Haar cascades for facial detection.<ul><li><p>Only frontal faces are detected<li><p>The resolution of faces in the video is very low</ul><h1 id="full-body-features">Full Body Features</h1><h2 id="pedestrian-detection">Pedestrian Detection</h2><p>There are various difficulties in localizing pedestrians in an image. These can be broadly classified into mainly Illumination, Occlusion, scales, clutter in the background, variable appearances and the different human poses. We have used the famous Histogram of Oriented Gradients <sup id="fnref:dalal2005histograms"><a href="#fn:dalal2005histograms" class="footnote">7</a></sup> for Human detection. The pipeline briefly has the following steps: Scan image at all possible scales and locations, then extract features over these windows and run Linear SVM , then fuse the multiple detections that we have found in 3D space for scale and position and we finally get the true bounding box for the detections. Dalal tried to optimize each of these steps and used every possible regularization at each step. For the learning part, first a fixed resolution and normalized training image data set is created and after encoding images into feature spaces, a binary classifier is learnt. <sup id="fnref:PapEvg98"><a href="#fn:PapEvg98" class="footnote">8</a></sup><h2 id="data-set-used-1">Data Set Used</h2><p><img src="http://localhost:4000/assets/posts/2016-04-09-can-we-discriminate/24.png" alt="image" /> MIT Pedestrian Dataset was used<sup id="fnref:mitdataset"><a href="#fn:mitdataset" class="footnote">9</a></sup><h2 id="results">Results</h2><table><thead><tr><th>Classifier<th>HOG Descriptor<th>Pixel Values<tbody><tr class="odd"><td>LinearSVC<td><span class="math inline">0.75 ± 0.04</span><td><span class="math inline">0.63 ± 0.05</span><tr class="even"><td>Random Forest<td><span class="math inline">0.67 ± 0.04</span><td><span class="math inline">0.60 ± 0.05</span><tr class="odd"><td>Perceptron<td><span class="math inline">0.69 ± 0.09</span><td><span class="math inline">0.63 ± 0.09</span><tr class="even"><td>Adaboost<td><span class="math inline">0.71 ± 0.05</span><td><span class="math inline">0.65 ± 0.04</span><tr class="odd"><td>MultinomialNB<td><span class="math inline">0.63 ± 0.04</span><td><span class="math inline">0.53 ± 0.04</span></table><h2 id="online-port-1">Online Port</h2><p><img src="http://localhost:4000/assets/posts/2016-04-09-can-we-discriminate/12.png" alt="image" /><h2 id="demonstration-1">Demonstration</h2><p>As the human recognition algorithm detects humans in general, it could detect people driving their bycycles too. It can bee seen that this was very much a possible result. The redbox around the woman’s body indicates that here sex is female based on her full body features.<div> <img src="http://localhost:4000/assets/posts/2016-04-09-can-we-discriminate/42.png" /></div><h1 id="motion-based-features">Motion Based Features</h1><p>Gait<sup id="fnref:han2006individual"><a href="#fn:han2006individual" class="footnote">10</a></sup> is defined as the combination of coordinated cyclic movements are that result in human locomotion, which includes walking, running, jogging and climbing stairs. Gait of a person on foot is often used to exploiting process information in some situations, for example when the face is not visible. In a video of a person walking, the gait cycle can be designated as the time interval between two positions. Many factors affect the approach of a person, such as the load, shoes, walking area, injury, mood, age and change over time. Video analysis of this process will also deal with clothes for the camera, walking speed and clutter background.<p>For our purpose gait images are background subtracted images of persons motion across a scene at different times in an interval.<p><img src="http://localhost:4000/assets/posts/2016-04-09-can-we-discriminate/10.jpg" alt="image" /> Data Set used is CASIA Gait Data Set<sup id="fnref:gaitdataset"><a href="#fn:gaitdataset" class="footnote">11</a></sup><p>From these gait images we’ll extract only the person/silhouette as it is the only part that holds any feature relevant to our problem.<h2 id="gait-energy-image">Gait Energy Image</h2><p>We define Gait Energy Image as the average of silhouettes in a gait/walk cycle.<sup id="fnref:yu2006framework"><a href="#fn:yu2006framework" class="footnote">12</a></sup> <script type="math/tex; mode=display">F(i,j) = \frac{1}{T}\sum_{t=1}^{T}I(i,j,t)</script><p>where $T$ is the number of frames in the sequence $I(i,j)$, $I(i,j,t)$ is the binary silhouttes image pixel at frame $t(i,j)$ with $i,j$ as coordinates.<p><img src="http://localhost:4000/assets/posts/2016-04-09-can-we-discriminate/7.png" alt="image" /><p><img src="http://localhost:4000/assets/posts/2016-04-09-can-we-discriminate/8.png" alt="image" /><p>These are the extracted Gait Images. The last image in each row is the Gait Enery Image of the corresponding row.<h2 id="classifiers">Classifiers</h2><p>We’ve used multiple classifiers with GEI as the input vectors. SVM as was expected is a very good classifiers for this two class problem.<table><thead><tr><th>Classifier<th>HOG Descriptor<th>PCA Reduction<th>Raw GEI Values<tbody><tr class="odd"><td>LinearSVC<td><span class="math inline">0.83 ± 0.008</span><td><span class="math inline">0.90 ± 0.006</span><td><span class="math inline">0.91 ± 0.004</span><tr class="even"><td>Random Forest<td><span class="math inline">0.60 ± 0.008</span><td><span class="math inline">0.75 ± 0.020</span><td><span class="math inline">0.80 ± 0.030</span><tr class="odd"><td>Perceptron<td><span class="math inline">0.74 ± 0.030</span><td><span class="math inline">0.85 ± 0.010</span><td><span class="math inline">0.60 ± 0.030</span><tr class="even"><td>Adaboost<td><span class="math inline">0.66 ± 0.020</span><td><span class="math inline">0.79 ± 0.009</span><td><span class="math inline">0.82 ± 0.000</span></table><h2 id="online-port-2">Online Port</h2><p><img src="http://localhost:4000/assets/posts/2016-04-09-can-we-discriminate/13.png" alt="image" /><h2 id="problem-with-an-online-version">Problem with an online version</h2><ul><li><p>To generate a Gait Energy Image, we need a set of gaits of the person.<li><p>The problem with generating such gaits, lies on three fronts.<li><p>Detection of Person<li><p>Background Subtraction<li><p>Tracking</ul><h2 id="contribution-of-halos-and-cores-in-gait-energy-images">Contribution of Halos and Cores in Gait Energy Images</h2><div class="row"><div class="col-md-6"><p>The images to right are normal gaits extracted. If we use these directly the accuracy is a bit low  70$\%$. This might be because we are unable to quantify what stride to use.</div><div class="col-md-6"> <img src="http://localhost:4000/assets/posts/2016-04-09-can-we-discriminate/31.png" /></div></div><div class="row"><div class="col-md-6"><p>The images to right are Average gaits. As was reported they provide an accuracy of  91$\%$.</div><div class="col-md-6"><img src="http://localhost:4000/assets/posts/2016-04-09-can-we-discriminate/32.png" /></div></div><div class="row"><div class="col-md-6"><p>The images to right are Halo subtracted Average gaits (Cores). These seem to capture the body features of an individual in a normalized fashion. <br /> They surprisingly provide us with an accuracy of  87$\%$</div><div class="col-md-6"><img src="http://localhost:4000/assets/posts/2016-04-09-can-we-discriminate/33.png" /></div></div><div class="row"><div class="col-md-6"><p>The images to right are just the Halos around the cores, they capture the body motions effectively. <br /> Even they provide us an accuracy of  86$\%$.</div><div class="col-md-6"><img src="http://localhost:4000/assets/posts/2016-04-09-can-we-discriminate/34.png" /></div></div><p><br /> As we can see the people’s strides could be random in single images but on an average, it incorporates all the information about the pattern of movement, thus Gait images provide us with good results.<p>The results above also suggest that the Halos as well as the cores independently are capable of describing a person, with notable accuracy (&gt; 85$\%$). When used together as is the case in Average gait image, the information about an individual seems to have been boosted, hence greater success rates.<h2 id="explained-variance-for-pca-reduction">Explained Variance for PCA reduction</h2><p><img src="http://localhost:4000/assets/posts/2016-04-09-can-we-discriminate/9.jpg" alt="image" /> Optimal Value of N turns out to be close to 30\ This gives us an idea that the entire motion can most probably be described in 30 features.<p>Here’s a presentation… <iframe style="width: 100% !important; min-height:600px;" src="https://manikantareddyd.github.io/slides/Sex%20Classification/slides-gender-classification.html#/"></iframe> <center><h2> Fork the code</h2><a href="https://github.com/HMAP/Online_Pedestrian_Sex_Identification" target="_blank"><i class="fa fa-3x fa-github"></i></a><br /> and a <a href="http://localhost:4000/reports/A Survey on Human Sex Determination Methods.pdf" target="_blank"><button class="btn btn-default">Report</button></a> </center><h2 id="references">References</h2><div class="footnotes"><ol><li id="fn:ng2012vision"><p>Choon Boon Ng, Yong Haur Tay, and Bok Min Goi. Vision-based human gender recognition: A survey. arXiv preprint arXiv:1204.1611, 2012.&nbsp;<a href="#fnref:ng2012vision" class="reversefootnote">&#8617;</a><li id="fn:viola2001rapid"><p>Paul Viola and Michael Jones. Rapid object detection using a boosted cascade of simple features. In Computer Vision and Pattern Recognition, 2001. CVPR 2001. Proceedings of the 2001 IEEE Computer Society Conference on, volume 1, pages I–511. IEEE, 2001.&nbsp;<a href="#fnref:viola2001rapid" class="reversefootnote">&#8617;</a><li id="fn:viola2004robust"><p>Paul Viola and Michael J Jones. Robust real-time face detection. International journal of computer vision, 57(2):137–154, 2004.&nbsp;<a href="#fnref:viola2004robust" class="reversefootnote">&#8617;</a><li id="fn:jolliffe2002principal"><p>Ian Jolliffe. Principal component analysis. Wiley Online Library, 2002.&nbsp;<a href="#fnref:jolliffe2002principal" class="reversefootnote">&#8617;</a><li id="fn:turk1991face"><p>Matthew A Turk and Alex P Pentland. Face recognition using eigenfaces. In Computer Vision and Pattern Recognition, 1991. Proceedings CVPR’91., IEEE Computer Society Conference on, pages 586–591. IEEE, 1991.&nbsp;<a href="#fnref:turk1991face" class="reversefootnote">&#8617;</a><li id="fn:lowe2004distinctive"><p>David G Lowe. Distinctive image features from scale-invariant keypoints. International journal of computer vision, 60(2):91–110, 2004.&nbsp;<a href="#fnref:lowe2004distinctive" class="reversefootnote">&#8617;</a><li id="fn:dalal2005histograms"><p>Navneet Dalal and Bill Triggs. Histograms of oriented gradients for human detection. In Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference on, volume 1, pages 886–893. IEEE, 2005.&nbsp;<a href="#fnref:dalal2005histograms" class="reversefootnote">&#8617;</a><li id="fn:PapEvg98"><p>C. Papageorgiou, T. Evgeniou, and T. Poggio. A trainable pedestrian detection system. In Proceeding of Intelligent Vehicles, pages 241–246, October 1998.&nbsp;<a href="#fnref:PapEvg98" class="reversefootnote">&#8617;</a><li id="fn:mitdataset"><p>MIT. MIT Pedestrian Data Set. http://cbcl.mit.edu/software-datasets/PedestrianData.html. Accessed: 2016-04-01.&nbsp;<a href="#fnref:mitdataset" class="reversefootnote">&#8617;</a><li id="fn:han2006individual"><p>Jinguang Han and Bir Bhanu. Individual recognition using gait energy image. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 28(2):316–322, 2006.&nbsp;<a href="#fnref:han2006individual" class="reversefootnote">&#8617;</a><li id="fn:gaitdataset"><p>Chinese Academy of Sciences (CASIA) Institute of Automation. CASIA Gait Dataset. http://www.cbsr.ia.ac.cn/english/Gait%20Databases. asp. Accessed: 2016-04-05.&nbsp;<a href="#fnref:gaitdataset" class="reversefootnote">&#8617;</a><li id="fn:yu2006framework"><p>Shiqi Yu, Daoliang Tan, and Tieniu Tan. A framework for evaluating the effect of view angle, clothing and carrying condition on gait recognition. In Pattern Recognition, 2006. ICPR 2006. 18th International Conference on, volume 4, pages 441–444. IEEE, 2006.&nbsp;<a href="#fnref:yu2006framework" class="reversefootnote">&#8617;</a></ol></div><hr /><div class="row comments animated fadeInUp"><h3>Comments</h3><button id="show-comments" class="btn btn-primary">Load comments</button><div id="comments-spinner" style="display: none;"><!--<div class="spinner"><div class="rect1"></div><div class="rect2"></div><div class="rect3"></div><div class="rect4"></div><div class="rect5"></div></div>--><div style="text-align: center;"> <svg width="33%" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 100 100" preserveAspectRatio="xMidYMid" class="lds-infinity"><path fill="none" ng-attr-stroke="" ng-attr-stroke-width="" ng-attr-stroke-dasharray="" d="M24.3,30C11.4,30,5,43.3,5,50s6.4,20,19.3,20c19.3,0,32.1-40,51.4-40 C88.6,30,95,43.3,95,50s-6.4,20-19.3,20C56.4,70,43.6,30,24.3,30z" stroke="#03a9f4" stroke-width="1" stroke-dasharray="6.414723205566406 6.414723205566406"> <animate attributeName="stroke-dashoffset" calcMode="linear" values="0;256.58892822265625" keyTimes="0;1" dur="5" begin="0s" repeatCount="indefinite"></animate> </path> </svg></div></div><div id="comments" style="display: none;"><ul class="nav nav-pills"></ul><hr /><div class="tab-content"></div></div></div><h2 style="text-align: center;">More Posts</h2><div class="row"><div class="col-md-6"><div class="card card-raised"><div class="card-image" style="background-image: url('/assets/posts/2016-04-11-cats-or-dogs/feature.jpg'); "></div><div class="content"> <a href="http://localhost:4000/blog/2016/04/11/cats-or-dogs/"><h3>Cats or Dogs</h3></a><div class="row"><p class="col-md-6">11 April 2016<p class="col-md-6">Reading time ~4 minutes</div><p>In this experiment we'll train a machine to distinguish between cats and dogs. We'll be using Support vector machines for the same.</div><div class="tags content-bottom"> <button class="btn btn-simple btn-primary" style="padding: 5px;"><a href="http://localhost:4000/tags/ai" class="">ai</a></button> <button class="btn btn-simple btn-primary" style="padding: 5px;"><a href="http://localhost:4000/tags/machine-learning" class="">machine-learning</a></button> <button class="btn btn-simple btn-primary" style="padding: 5px;"><a href="http://localhost:4000/tags/binary-classification" class="">binary-classification</a></button> <button class="btn btn-simple btn-primary" style="padding: 5px;"><a href="http://localhost:4000/tags/random-forests" class="">random-forests</a></button></div></div></div><div class="col-md-6"><div class="card card-raised"><div class="card-image" style=" "><h2>Previous</h2></div><div class="content"> <a href="http://localhost:4000/blog/2016/03/08/visual-words/"><h3>Visual Word Representation</h3></a><div class="row"><p class="col-md-6">08 March 2016<p class="col-md-6">Reading time ~2 minutes</div><p>Do images speak?</div><div class="tags content-bottom"> <button class="btn btn-simple btn-primary" style="padding: 5px;"><a href="http://localhost:4000/tags/visual-words" class="">visual-words</a></button> <button class="btn btn-simple btn-primary" style="padding: 5px;"><a href="http://localhost:4000/tags/sift" class="">sift</a></button> <button class="btn btn-simple btn-primary" style="padding: 5px;"><a href="http://localhost:4000/tags/image-processing" class="">image-processing</a></button></div></div></div></div></div></div></div></div></div><!-- Core JS Files --> <script src="http://localhost:4000/assets/javascripts/vendors/jquery.min.js" type="text/javascript"></script> <script src="http://localhost:4000/assets/javascripts/vendors/material-kit/bootstrap.min.js" type="text/javascript"></script> <script src="http://localhost:4000/assets/javascripts/vendors/material-kit/material.min.js"></script><!-- <script src="http://localhost:4000/assets/javascripts/vendors/material-kit/nouislider.min.js" type="text/javascript"></script> --> <script src="http://localhost:4000/assets/javascripts/vendors/material-kit/bootstrap-datepicker.js" type="text/javascript"></script> <script src="http://localhost:4000/assets/javascripts/vendors/material-kit/material-kit.js" type="text/javascript"></script> <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML"></script> <script type="text/javascript"> MathJax.Hub.Config({ tex2jax: { inlineMath: [ ['$','$'], ['\\(','\\)'] ] } }); </script><!-- <script type="text/javascript"> $().ready(function () { materialKit.initSliders(); window_width = $(window).width(); if (window_width >= 992) { big_image = $('.wrapper > .header'); $(window).on('scroll', materialKitDemo.checkScrollForParallax); } }); </script> --><div id="fb-root"></div><script> var load_facebook = function(){ (function (d, s, id) { var js, fjs = d.getElementsByTagName(s)[0]; if (d.getElementById(id)) return; js = d.createElement(s); js.id = id; js.src = "//connect.facebook.net/en_US/sdk.js#xfbml=1&version=v2.5&appId="; fjs.parentNode.insertBefore(js, fjs); }(document, 'script', 'facebook-jssdk')); }; </script> <script src="http://localhost:4000/assets/javascripts/comments.js"></script> <script> $(document).on('click', 'a[href^="#"]', function (event) { event.preventDefault(); $('html, body').animate({ scrollTop: $($.attr(this, 'href')).offset().top -150 }, 500); }); </script>
    